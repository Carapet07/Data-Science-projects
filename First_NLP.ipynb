{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carapet07/Data-Science-projects/blob/main/First_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7qqSjS52gJ5"
      },
      "source": [
        "##My first NLP project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj4RY4Norqpb"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFU6Gvlrrm0Z",
        "outputId": "8eac67e0-28d3-449b-9249-109802940f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  1703k      0  0:00:48  0:00:48 --:--:-- 2141k\n",
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcjZF4pMsS8i"
      },
      "outputs": [],
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "# Move 20% of the training data into new validation directory\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files) # shuffle files for randomness\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname,\n",
        "                val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HykwDKwxvtqe",
        "outputId": "3780b253-7b93-4e77-98e4-772faa3c8794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Create datasets from directories, automatically batching text samples\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n4BzlrG1S-8",
        "outputId": "da19937a-8946-40bb-ce43-0ce60b6e88b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Genghis Cohn is a (very) mildly entertaining British movie about a German police commissioner in the late 1950's who is haunted by the ghost of a Jewish comedian that he killed 15 years earlier while serving under Hitler in the SS. The ghost comes back and wants his killer to live as a Jew to atone for the murders he committed.<br /><br />Otto, the German policeman actually knows this ghost's name because, the last thing he did before he died was said, in Yiddish, `Kiss my ass'. The policeman didn't speak Yiddish, so he asked around until he found the meaning. The `kiss my ass' left such an impression that everybody involved with that killing learned and remembered the comedian's name, Genghis Cohn.<br /><br />There are a bunch of men who are murdered in the jurisdiction of the police commissioner, and there are no helpful clues. The men are murdered with a set of knives that are missing from the local butcher. The butcher announces that his knives are missing while the commissioner is in the store to get a liver and onion sandwich, so the commissioner is a suspect. The first man is killed while making love to the butcher's wife, so the butcher is a suspect. But the butcher maintains that he would be very busy if he killed every man that slept with his wife. All the men are killed immediately after the climax of lovemaking.<br /><br />I think I might be a bit angrier than the ghost of Genghis Cohn if I was killed like he was. He seems to be very good-natured about it, as if he was just in a mild car accident. I can only guess that it is because it is a British movie and they are known for being a very polite people. He uses some of his material from his stand-up routing, and I just didn't find it very funny.<br /><br />I gave this movie a 4 because it was just kind of goofy. I thought it should have been a little more serious than it was. The movie turns out to be a murder mystery (where did this come from?), and it seemed that Genghis should have been more helpful than he was. The movie gave me a tiny look into Jewish culture, but was only skin-deep. Do all Jews love liver and onion sandwiches? Do they all say `shtoop' and `meshuganah' in their daily vocabulary? Isn't there more important stuff that we should know about the culture?<br /><br />I saw this movie at a Jewish community center in Berkeley, CA, and I was the only person in the room whose hair was not fully gray or white. (I have no gray or white hair.) There were 18 of us, and after the movie they stayed for about 20 minutes to discuss the movie. There were 2 main concerns expressed there: 1. The movie was way too light-hearted and future generations might not understand the gravity of what happened and 2. As the Holocaust survivors are dying off, future generations will not know what really happened. I thought that this second concern was ridiculous and I told them I thought they didn't need to worry because there is tons of literature out there and there will always be people who like to watch movies, like myself. The murder of 6,000,000 people by a very bad man will not ever be forgotten. I write this last paragraph because they charged me with telling others about my experience that day.\n",
            "Label: 0\n",
            "==================================================\n",
            "Text: Jim Carrey and Morgan Freeman along with Jennifer Aniston combine to make one of the funniest movies so far this 2003 season (late May) and a good improvement on Carrey's past crazy and personally forgetable roles in past comedies. With a slightly toned down Carrey antics yet with just the zap and crackle of his old self, Carrey powerfully carries this movie to the height of laughter and also some dramatic, tearfully somber moments. Elements of Jim's real acting abilities continue to show up in this movie. This delightful summer entertainment hits most of the buttons, including dramatic elements along with the goofy moments that fit perfectly with this script. While still lacking in the superbly polished ensemble of comedy/drama, Bruce, Almightly deserves credit for being a great date movie along with a solid message and soft spiritual cynicism and parody that maintains its good-natured taste. Eight out of ten stars.\n",
            "Label: 1\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):  # Take one batch\n",
        "    for i in range(2):  # Show the first 5 examples\n",
        "        print(f\"Text: {text_batch.numpy()[i].decode('utf-8')}\")\n",
        "        print(f\"Label: {label_batch.numpy()[i]}\")\n",
        "        print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbs7ryTqs5j-"
      },
      "outputs": [],
      "source": [
        "max_length = 600 # max length of a movie review\n",
        "max_tokens = 20000 # max number of unique words to consider\n",
        "\n",
        "# Create a TextVectorization layer that converts text into integer sequences\n",
        "# eg: \"dog\": [0.2, 0.5, -0.7, 0.21... embedding_dim]\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max_length\n",
        ")\n",
        "\n",
        "# Extract only the text fron train_ds, removing lables (0, 1)\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# This line adapts the TextVectorization layer to the dataset, learning the vocabulary\n",
        "# by assigning each word in the dataset a unique integer index\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# apply it to every dataset we have\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y),\n",
        "                            num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),\n",
        "                            num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y),\n",
        "                            num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEyeDCVK0-0p",
        "outputId": "99d840d1-6df7-484e-ef73-5439829ac63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape:  (32,)\n",
            "input dtype:  <dtype: 'string'>\n",
            "targets shape:  (32,)\n",
            "targets dtype:  <dtype: 'int32'>\n",
            "inputs[0]:  tf.Tensor(b\"There is no doubt that this film has an impressive cast but unfortunately this doesn't help with the major downsides to the movie. I never understand why directors ask actors/actresses to use accents not their own when it is obvious to everyone they can't convince. Fiennes just can't do Irish and Fitzgerald isn't much better at Russian. When the voice is wrong then no matter how good the acting the character will never be convincing. As the for the major problem....the plot....was there one? I guess there was some sort of storyline involved but it was so full of holes that I just couldn't wait for the film to end...it was ridiculous. Save 90 minutes of your life and don't watch this movie!\", shape=(), dtype=string)\n",
            "targets[0]:  tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"input shape: \", inputs.shape)\n",
        "  print(\"input dtype: \", inputs.dtype)\n",
        "  print(\"targets shape: \", targets.shape)\n",
        "  print(\"targets dtype: \", targets.dtype)\n",
        "  print('inputs[0]: ', inputs[0])\n",
        "  print('targets[0]: ', targets[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhGZLkKnoX98",
        "outputId": "afd04e6a-952f-468f-aba5-1a1042863432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-16 06:40:49--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-02-16 06:40:49--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-02-16 06:40:50--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.63MB/s    in 3m 0s   \n",
            "\n",
            "2025-02-16 06:43:50 (4.57 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# downlaod GloVe precomputed database for words ebedding task\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNirX4xD4PNr"
      },
      "source": [
        "Parse the unzipped file (.txt file) to build an index that maps words(as strings) to their words reprezentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH_Kyi3rqD61",
        "outputId": "6ce2acc3-71aa-4fe0-f274-f8fc3df76a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found 400000 words\n"
          ]
        }
      ],
      "source": [
        "glove_100d_path = \"glove.6B.100d.txt\"\n",
        "# 1: Create an embedding dictionary\n",
        "embeddings_index = {}\n",
        "\n",
        "# 2: Parse the GloVe txt file. Add each word and it's embedding into emeddings_index dictionary\n",
        "with open(glove_100d_path) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \") # converts coefs strings into float32\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "print(f'found {len(embeddings_index)} words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEXtN25I4cdl"
      },
      "source": [
        "Next lets build an embedding matrix that will be loaded into an Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xMKTOSgB5NoD"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# 1: Get the vocabulary from the TextVectorizaton layer\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "\n",
        "# 2: Assign to each word a uniquie number\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "# 3: Create a matrix filled with zeros\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "\n",
        "# 4: Populate embedding_matrxi with GloVe embeddinngs for each word in the vocabulary\n",
        "for word, i in word_index.items():\n",
        "  if i < max_tokens:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ac6wFVefSTt"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkkTRznT2IO5"
      },
      "source": [
        "This dataset yields inputs that are tf.string tensors and targets that are int32 tensors encoding the value \"0\" or \"1\"\n",
        "\n",
        "\n",
        "All set, now let's try learnign something from this data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VVVXhop2EXP"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None, ), dtype='int64')\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJrWeNRPrubQ"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                      save_best_only=True)\n",
        "    ]\n",
        "\n",
        "history = model.fit(int_train_ds.cache(),\n",
        "  validation_data=int_val_ds.cache(),\n",
        "  epochs=10,\n",
        "  callbacks=callbacks)\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMfxTK1DfIpsW+fa13cwb9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}