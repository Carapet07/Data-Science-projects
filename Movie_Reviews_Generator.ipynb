{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfbTlnQrwxqZt2xLf+/dO5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carapet07/Data-Science-projects/blob/main/Movie_Reviews_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "BRtZ7-32dn6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-IYwZygdBOr",
        "outputId": "517bfd16-3700-4d04-8f3b-8fc64e4bf7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-01 17:08:17--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  9.61MB/s    in 6.8s    \n",
            "\n",
            "2025-03-01 17:08:24 (11.7 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory='aclImdb', label_mode=None,  batch_size=256\n",
        ") # Only raw data is returned because of label_mode='None'\n",
        "\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))\n",
        "# Imdb dataset often contain <br /> for line breaks. This doesn't matter for text\n",
        "# classification, but in our case we wouldn't want to generate <br />"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPOmROUJdmgO",
        "outputId": "4708584e-8fed-4ecf-8ef1-4c80532384f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 15000\n",
        "sequence_length = 100\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocabulary_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length = sequence_length\n",
        ")\n",
        "\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "5ECccW9IfZaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "  vectorized_sequence = text_vectorization(text_batch)\n",
        "\n",
        "  X = vectorized_sequence[:, :-1]\n",
        "  y = vectorized_sequence[:, 1:]\n",
        "\n",
        "  return X, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "PNuORR0QgILt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, dense_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.dense_dim = dense_dim\n",
        "\n",
        "    self.dense = keras.Sequential([\n",
        "        layers.Dense(dense_dim, activation='relu'),\n",
        "        layers.Dense(embed_dim)\n",
        "    ])\n",
        "\n",
        "    self.attention = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "\n",
        "    self.layernorm1 = layers.LayerNormalization()\n",
        "    self.layernorm2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "    attention_output = self.attention(\n",
        "        inputs, inputs, attention_mask=mask\n",
        "    )\n",
        "\n",
        "    dense_input = self.layernorm1(inputs + attention_output)\n",
        "    dense_output = self.dense(dense_input)\n",
        "    return self.layernorm2(dense_input + dense_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    # Embeddin for storing token indices\n",
        "    self.token_embedding = layers.Embedding(\n",
        "        input_dim=input_dim, output_dim=output_dim,\n",
        "    )\n",
        "    # Embeddin for saving token positions\n",
        "    self.positional_embedding = layers.Embedding(\n",
        "        input_dim=sequence_length, output_dim=output_dim\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # inputs is a tensor of shape (batch_size, sequence_length)\n",
        "    # so [-1] returns sequence_length\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_positions = self.positional_embedding(positions)\n",
        "    embedded_tokens = self.token_embedding(inputs)\n",
        "    return embedded_positions + embedded_tokens\n",
        "\n",
        "  def mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)"
      ],
      "metadata": {
        "id": "qrwgzpq22Mgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs =  keras.Input(shape=(None,), dtype='int64')\n",
        "x = PositionalEmbedding(sequence_length, vocabulary_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, num_heads, dense_dim)(x, x)\n",
        "outputs = layers.Dense(vocabulary_size, activation='softmax')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"
      ],
      "metadata": {
        "id": "zL53s_iY9OdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}